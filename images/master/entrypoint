#!/usr/bin/env bash

#
# Setup user accounts with proper bash profiles
#
chmod +x /etc/docker/setup
if [[ "$HADOOP_AUTHENTICATION" == "kerberos" ]]; then
    #                 user   group   domain
    /etc/docker/setup hdfs   hadoop  docker.net
    /etc/docker/setup mapred hadoop  docker.net
    /etc/docker/setup yarn   hadoop  docker.net
fi


#
# Render configuration templates by replacing {{ PLACEHOLDER }}
# with corresponding shell environment variable values
chmod +x /etc/docker/render
/etc/docker/render  /etc/hadoop/conf/core-site.xml
/etc/docker/render  /etc/hadoop/conf/hdfs-site.xml
/etc/docker/render  /etc/hadoop/conf/mapred-site.xml
/etc/docker/render  /etc/hadoop/conf/yarn-site.xml
# TODO /etc/docker/render  /etc/hadoop/conf/hive-site.xml


#
# Make Hadoop DFS log directories
#
mkdir -p /var/log/hadoop-hdfs
chown hdfs:hdfs /var/log/hadoop-hdfs
chmod 755 /var/log/hadoop-hdfs

#
# Make Hadoop MapReduce log directories
#
mkdir -p /var/log/hadoop-mapreduce
chown mapred:mapred /var/log/hadoop-mapreduce
chmod 755 /var/log/hadoop-mapreduce

#
# Make Hadoop YARN log directories
#
mkdir -p /var/log/hadoop-yarn
chown -R yarn:yarn /var/log/hadoop-yarn
chmod 755 /var/log/hadoop-yarn


#
# (Maybe) Format the Hadoop HDFS filesystem
#
# export HADOOP_DFS_DROP=${HADOOP_DFS_DROP:-'yes'}
# if [[ "${HADOOP_DFS_DROP}" == "yes" ]]; then
  mkdir -p /data/1/hdfs/nn
  hdfs namenode -format -force
  chown -R hdfs:hdfs /data/1/hdfs/nn
  chmod 700 /data/1/hdfs/nn
# fi

#
# Start namenode service
#
service hadoop-hdfs-namenode start

#
# Wait until namenode service is available
#
while ! echo exit | nc namenode.docker.net 8020; do sleep 1; done

#
# Initialize the Hadoop DFS with fundamental directories
#
# If you do not create /tmp properly, with the right permissions as shown below,
# you may have problems with CDH components later. Specifically, if you do not
# create /tmp yourself, another process may create it automatically with
# restrictive permissions that will prevent your other applications from using it.
#
su - hdfs <<'EOF'
hadoop fs -chown hdfs:hadoop /
hadoop fs -chmod 755 /

hadoop fs -mkdir -p /tmp
hadoop fs -chmod -R 1777 /tmp

hadoop fs -mkdir -p /user/hive/warehouse
hadoop fs -chmod -R 1777 /user/hive/warehouse
EOF

# YARN requires a staging directory for temporary files created by running jobs.
# By default it creates /tmp/hadoop-yarn/staging with restrictive permissions
# that may prevent your users from running jobs. To forestall this, you should
# configure and create the staging directory yourself.
#
su - yarn <<'EOF'
hadoop fs -mkdir -p /user/history
hadoop fs -chown mapred:hadoop /user/history
hadoop fs -chmod -R 1777 /user/history
EOF


#
# Start the Hadoop MapReduce JobHistory service
#
service hadoop-mapreduce-historyserver start

#
# Start the Hadoop YARN ResourceManager service
#
service hadoop-yarn-resourcemanager start

#
# Keep this container running
#
/etc/docker/idle
