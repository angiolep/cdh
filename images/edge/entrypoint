#!/usr/bin/env bash

#
# Setup user accounts with proper ~/.bash_profile
#
chmod +x /etc/docker/setup
# --------------   user   group   domain
/etc/docker/setup  alice  hadoop
/etc/docker/setup  david  hadoop



#
# Render configuration templates by replacing {{ PLACEHOLDER }}
# with corresponding shell environment variable values
#
chmod +x /etc/docker/render
/etc/docker/render  /etc/hadoop/conf/core-site.xml
/etc/docker/render  /etc/hadoop/conf/hdfs-site.xml
/etc/docker/render  /etc/hadoop/conf/mapred-site.xml
/etc/docker/render  /etc/hadoop/conf/yarn-site.xml
/etc/docker/render  /etc/hive/conf/hive-site.xml


# Wait until both namenode and datanode services are available
while ! echo exit | nc namenode.docker.net 8020; do sleep 10; done
# TODO while ! echo exit | nc datanode1.docker.net 1004; do sleep 10; done
# TODO careful with port number 1004


# Create data files and sleep
su - alice <<'EOF'
hadoop fs -mkdir -p /data/biostats
hadoop fs -chmod -R 775 /data

hadoop fs -copyFromLocal -f /etc/docker/titanic.csv  /data
hadoop fs -chmod -R 664 /data/titanic.csv

hadoop fs -copyFromLocal -f /etc/docker/biostats.csv /data/biostats
hadoop fs -chmod -R 664 /data/biostats/biostats.csv
EOF

sleep 8

# Create Hive tables and sleep
su - alice <<'EOF'
beeline \
  -u $HIVE2_URL \
  -n alice -p bogus \
  -f /etc/docker/create.hql
EOF


# Publish the Hadoop configuration through the /shared/conf directory
mkdir -p /shared/conf
rm -Rf /shared/conf/*
cp /etc/hadoop/conf/core-site.xml /shared/conf
cp /etc/hadoop/conf/hdfs-site.xml /shared/conf
cp /etc/hadoop/conf/mapred-site.xml /shared/conf
cp /etc/hadoop/conf/yarn-site.xml /shared/conf
cp /etc/hive/conf/hive-site.xml /shared/conf

# Publish the Kerberos configuration too
cp /etc/krb5.conf /shared/conf


# Start the SSH deamon service
service sshd start


# Keep this container running
/etc/docker/idle
