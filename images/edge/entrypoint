#!/usr/bin/env bash

# Cleanup previous configuration
rm -Rf /conf/*


chmod +x /etc/docker/setup
chmod +x /etc/docker/render

export HADOOP_AUTHENTICATION=${HADOOP_AUTHENTICATION:-'simple'}
if [[ "$HADOOP_AUTHENTICATION" == "kerberos" ]]; then
  #                  user   group   domain
  /etc/docker/setup  alice  hadoop
  /etc/docker/setup  bob    hadoop
  /etc/docker/setup  spark  hadoop  docker.net
fi

#
# Render configuration templates by replacing {{ PLACEHOLDER }}
# with corresponding shell environment variable values
#
/etc/docker/render  /etc/hadoop/conf/core-site.xml
/etc/docker/render  /etc/hadoop/conf/hdfs-site.xml
/etc/docker/render  /etc/hadoop/conf/mapred-site.xml
/etc/docker/render  /etc/hadoop/conf/yarn-site.xml
# TODO /etc/docker/render  /etc/hadoop/conf/hive-site.xml


# Wait until both namenode and datanode services are available
while ! echo exit | nc namenode.docker.net 8020; do sleep 10; done
# TODO while ! echo exit | nc datanode1.docker.net 1004; do sleep 10; done
# TODO careful with port number 1004


su - alice <<'EOF'
hadoop fs -mkdir -p /data/raw
hadoop fs -chmod -R 775 /data
hadoop fs -copyFromLocal -f /var/data/raw/titanic.csv /data/raw
EOF


# Publish the Hadoop configuration through the shared /conf directory
files="core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml"
for file in $files; do
  cp /etc/hadoop/conf/$file /conf
done

# Publish the Kerberos configuration too
cp /etc/krb5.conf /conf


# Keep this container running
/etc/docker/idle
