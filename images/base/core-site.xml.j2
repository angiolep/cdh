<configuration>
  <property>
    <!--
      This property specifies the Hadoop NameNode and the default file system,
      in the form hdfs://<namenode host>:<namenode port>/. The default value
      is file///. The default file system is used to resolve relative paths;
      for example, if fs.defaultFS is set to hdfs://mynamenode/, the relative
      URI /mydir/myfile resolves to hdfs://mynamenode/mydir/myfile.

      Note: for the cluster to function correctly, the <namenode> part of the
      string must be the hostname (for example mynamenode), or the HA-enabled
      logical URI, not the IP address.
    -->
    <name>fs.defaultFS</name>
    <value>hdfs://namenode.docker.net:8020</value>
  </property>

  <!--
      Class for user to group mapping (get groups for a given user) for ACL.
      The default implementation, JniBasedUnixGroupsMappingWithFallback, will
      determine if the Java Native Interface (JNI) is available. If JNI is
      available the implementation will use the API within hadoop to resolve
      a list of groups for a user. If JNI is not available then the shell
      implementation, ShellBasedUnixGroupsMapping, is used. This implementation
      shells out to the Linux/Unix environment with the bash -c groups command
      to resolve a list of groups for a user.
  -->
  <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback</value>
  </property>


  {% if HADOOP_AUTHENTICATION == 'kerberos' %}
  <!-- See https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cdh_sg_hadoop_security_enable.html -->

  <!-- 	Enable Kerberos authentication -->
  <property>
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
  </property>

  <!--
    Controls whether tokens always use IP addresses. In secure multi-homed
    environments, the following parameter will need to be set to false
    (it is true by default) on both cluster servers and clients
  -->
  <property>
    <name>hadoop.security.token.service.use_ip</name>
    <value>false</value>
  </property>


  <!--
    authentication : authentication only
    integrity      : integrity check in addition to authentication
    privacy        : data encryption in addition to integrity
  -->
  <property>
    <name>hadoop.rpc.protection</name>
    <value>{{ SASL_PROTECTION | default('authentication') }}</value>
  </property>


  <!--
    The mapping from Kerberos principals to short names
  -->
  <property>
    <name>hadoop.security.auth_to_local</name>
    <value>
      RULE:[1:$1](.*)s/(.*)/$1/g
      RULE:[2:$1](.*)s/(.*)/$1/g
      DEFAULT
    </value>
  </property>


  <!--
    Hadoop allows you to configure proxy users to submit jobs or
    access HDFS on behalf of other users; this is called impersonation.
    When you enable impersonation, any jobs submitted using a proxy are
    executed with the impersonated user's existing privilege levels
    rather than those of a superuser.

    In the following configuration, alice is the impersonator (the real user)
    whereas bob, charlie and david are the subjects (the impersonatees)
  -->
  <property>
     <name>hadoop.proxyuser.alice.hosts</name>
     <value>*</value>
  </property>
  <property>
     <name>hadoop.proxyuser.alice.groups</name>
     <value>*</value>
  </property>
  <property>
     <name>hadoop.proxyuser.alice.users</name>
     <value>bob,charlie,david</value>
  </property>

  <!--
    And, in the following configuration, hive is the impersonator (the real user)
    whereas alice, bob, charlie and david are the subjects (the impersonatees)
  -->
  <property>
    <name>hadoop.proxyuser.hive.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hive.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hive.users</name>
    <value>alice,bob,charlie,david</value>
  </property>

  {% endif %}
</configuration>
